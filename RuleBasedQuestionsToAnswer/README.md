# RuleBasedQuestionsToAnswer
Rule based baseline which uses Tregex and SimpleNLG. Given a question and the answer string it generates a conversational reply containing the answer using linguistic rules

# Setup for RuleBasedQuestionsToAnswer
- Download [lexAccess2016lite](https://lexsrv3.nlm.nih.gov/LexSysGroup/Projects/lexAccess/2013+/release/lexAccess2016lite.tgz) and extract within `"RuleBasedQuestionsToAnswer"` directory.
- Download [simplenlg-v4.4.8](https://github.com/simplenlg/simplenlg/releases/download/v4.4.8/simplenlg-v4.4.8.zip) and extract within `"RuleBasedQuestionsToAnswer"` directory.
- Download [stanford-corenlp-full-2018-02-27](http://nlp.stanford.edu/software/stanford-corenlp-full-2018-02-27.zip) and extract within `"RuleBasedQuestionsToAnswer"` directory.

# Setup for feature based baselines `logistic_regression_baseline.py` and `softmax_baseline.py`:
- Create a directory `"LMs"` inside `"RuleBasedQuestionsToAnswer"` directory.
- Download [`lm_giga_64k_nvp_2gram`](https://www.keithv.com/software/giga/lm_giga_64k_nvp_2gram.zip), [`lm_giga_64k_nvp_3gram`](https://www.keithv.com/software/giga/lm_giga_64k_nvp_3gram.zip), [`lm_giga_64k_vp_2gram`](https://www.keithv.com/software/giga/lm_giga_64k_vp_2gram.zip) and [`lm_giga_64k_vp_3gram`](https://www.keithv.com/software/giga/lm_giga_64k_vp_3gram.zip) and extract all the LM models within the `"LMs"` folder
- Download and extract [preprocessed train test and val data files](https://mega.nz/#!wQR0jI7D!_mZo7vSQvwNvCjd9RMMbhuU77EPSRshwhCYLFWpOaKI) in `"train_data"` folder within the `"RuleBasedQeustionToAnswer"` parent directory.
## Dependencies:

### Feature extractor:
- `kenlm` - https://github.com/kpu/kenlm

# TODOs
- Convert the java code from hardcoded files to commandline arguments
- Create script that adds rules and negative samples to the train data generated by the `anaylse_mturk_data.py`
- Create feature extractor for the classifiers

## Problems with current OpenNMT decoding
- translate will only work with -batch_size 1
- training with sgd with standard parameters gave slightly lower training performance but better validation performance than adadelta
- After lot of experiments, sgd with lr 1.0 for 5 epochs and then 0.5 decay every next epoch seems to work the best

## What does each file do:
- `create_Opennmt_train_val_test_src_and_tgt_file_from_top_k_responses.py` - Takes the predictions generated from `get_top_k_responses_from_decomposable_attention_softmax_model.py` and stores them into relevant folders in `RuleBasedQuestionsToAnswer`. These files will be later used to train OpenNMT models
- `collect_passages_for_qas.py` - Takes the processed test files from `RuleBasedQuestionsToAnswer/squad_seq2seq_train_moses_tokenized` and adds the passage information for them
- `create_test_from_squad2_dev.py` - This will read the squad2 dev set and create a test set for the squad, coqa and Answer generation Seq2Seq models
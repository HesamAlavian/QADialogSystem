# We want to create a csv file for annoation from all the different system's output.
# First we will accumulate all the outputs of the different systems in a list.
# Then we will randomize/shuffle that list and write them in buckets of 10

import os
import csv
import ast
import random
import string
from sacremoses import MosesTokenizer
mt = MosesTokenizer()

random.seed(901)

def print_list(l):
	for e in l:
		print(e)
	print()

correct_answers_dict = dict()

def read_dev_test_q_and_a(src_dev_file):
	global correct_answers_dict
	# we will make a list of tuples containing ids, question and answer from the dev test src file
	q_list = list()
	q_set = set()
	with open(src_dev_file, "r") as reader:
		for i, line in enumerate(reader):
			q, a = line.strip().split(" ||| ")
			q_set.add(q)
			correct_answers_dict[q] = a
			q_list.append(((i+1), q, a))
	#NOTE: just checked. All questions are unique
	# print("set size:", len(q_set))
	return q_list

def read_decomposable_attn_output(decomposable_attn_model_output_file, id_q_a_list):
	# Since the number of question generated by decomposable attention model is less than the actual test set. 
	# We will keep track of the question for which this model couldn't generate a response
	q_dict = dict()
	with open(decomposable_attn_model_output_file, "r") as reader:
		line_tracker = 0
		instance_tracker = 1
		question, gold_answer, response = None, None, None
		for line in reader:
			line = line.strip()
			# first line is the question
			if line_tracker == 0:
				question = line
				line_tracker += 1
			# second line is the gold answer
			elif line_tracker == 1:
				gold_answer = line
				line_tracker += 1
			# Third line is the first/top ranked response from the model
			elif line_tracker == 2:
				_, response, _, _ = line.split("\t")
				# add to the dict here
				q_dict[question] = (gold_answer, response)
				line_tracker += 1
			if not line:
				# reset the line tracker for the next question
				line_tracker = 0
				instance_tracker += 1
	# Now we want to generate an id,q,a,r list
	# here we will simply copy the a into r if q is not present in the q_dict
	final_qar_list = list()
	found_count = 0
	for id, q, a in id_q_a_list:
		if q in q_dict:
			found_count += 1
			correct_a = correct_answers_dict[q]
			final_qar_list.append((id, q, correct_a, q_dict[q][1]))
		else:
			correct_a = correct_answers_dict[q]
			final_qar_list.append((id, q, correct_a, a))
	return final_qar_list

def read_coqa_output(coqa_output_file):
	# dictionary of id and question and response
	qr_dict = dict()

	with open(coqa_output_file, "r") as reader:
		current_id = -1
		for line in reader:
			line = line.strip()
			if line.startswith("Q "):
				current_id_str, question = line.split("\t\t:")
				new_id = int(current_id_str.replace("Q ", ""))
				# print(new_id)
				# print(question)
				if new_id != current_id:
					# print(current_id, new_id)
					current_id = new_id
				qr_dict[current_id] = question
			if line.startswith("Gold\t\t:"):
				gold_answer = line.replace("Gold\t\t:", "")
				qr_dict[current_id] = (qr_dict[current_id], gold_answer)
			if line.startswith("Coqa Pred\t:"):
				coqa_prediction_answer = line.replace("Coqa Pred\t:", "")
				qr_dict[current_id] = (*qr_dict[current_id], coqa_prediction_answer)
				# print(current_id, qr_dict[current_id])
	sorted_qr = sorted(qr_dict.items(), key=lambda kv: kv[0])
	final_qar_list = list()
	for id, (q, a, r) in sorted_qr:
		correct_a = correct_answers_dict[q]
		final_qar_list.append((id, q, correct_a, r))
	return final_qar_list

def read_opennmt_output(opennmt_output_file):
	# dictionary of id and question and response
	qr_dict = dict()

	with open(opennmt_output_file, "r") as reader:
		for line in reader:
			line = line.strip()
			line = line.replace("PRED SCORE: ", "")
			line = line.replace("PRED AVG SCORE: ", "")
			if line.startswith("SENT "):
				sent_part, src_list_part = line.split(": [")
				id = int(sent_part.replace("SENT ", ""))
				src_list_part = "[" + src_list_part
				src_list = ast.literal_eval(src_list_part)
				question = ' '.join(src_list[:src_list.index('|||')])
				answer = ' '.join(src_list[src_list.index('|||')+1:])
				qr_dict[id] = (question, answer)
			if line.startswith("PRED "):
				pred_part, response = line.split(": ", 1)
				id = int(pred_part.replace("PRED ", ""))
				qr_dict[id] = (*qr_dict[id], response)
	sorted_qr = sorted(qr_dict.items(), key=lambda kv: kv[0])
	final_qar_list = list()
	for id, (q, a, r) in sorted_qr:
		correct_a = correct_answers_dict[q]
		final_qar_list.append((id, q, correct_a, r))
		# print(id, q, ":::", a)
		# print(r)
	return final_qar_list

def read_quac_output(quac_output_file):
	final_qar_list = list()
	correct_answer_count = 0
	current_response = ""
	counter = 1
	with open(quac_output_file, "r") as reader:
		next_line_question = True
		for line in reader:
			line = line.strip()
			if line and next_line_question:
				current_q, current_a = line.split(" ||| ")
				next_line_question = False
			elif line and not next_line_question:
				current_response = line
				if current_a in current_response:
					correct_answer_count += 1
				else:
					# print(current_a, " :: ", current_response)
					pass
				current_response =  mt.tokenize(line[2:-2].strip(), return_str=True, escape=False).lower()
				# Remove punctuation at the end if present
				# 879
				if current_response[-1] in ['.',',',';']:
					current_response = current_response[:-1].strip()
				normalized_q = mt.tokenize(current_q.lower().strip(), return_str=True, escape=False)
				try:
					correct_a = correct_answers_dict[normalized_q]
				except KeyError:
					print_list(correct_answers_dict.keys())
					exit()
				final_qar_list.append((counter, normalized_q, correct_a, current_response))
			else:
				next_line_question = True
				counter += 1
	print("Correct quac answers:", correct_answer_count)
	return final_qar_list


DATA_FOLDER = "data"
src_test_file = os.path.join(DATA_FOLDER, "src_squad_seq2seq_dev_moses_test.txt")
decomposable_attn_model_output = os.path.join(DATA_FOLDER, "squad_seq2seq_dev_test_predictions_start_0_end_822.txt")
coqa_output = os.path.join(DATA_FOLDER, "coqa_predictions_on_squad_seq2seq_dev_moses_test.txt")
quac_output = os.path.join(DATA_FOLDER, "quac_answers_on_squad_dev_test.txt")
basic_output = os.path.join(DATA_FOLDER, "moses_try1_on_squad_dev_test_output.txt")
basic_and_pretraining_output = os.path.join(DATA_FOLDER, "pretrained_on_opensub_qa_moses_try1_output_on_squad_dev_test.txt")
basic_and_pretraining_and_glove_output = os.path.join(DATA_FOLDER, "pretrained_on_opensub_qa_moses_glove_try1_on_squad_dev_test_output.txt")
basic_and_pretraining_and_glove_and_lm_output = os.path.join(DATA_FOLDER, "pretrained_on_opensub_qa_moses_glove_try1_output_on_squad_dev_test_lm_reranked.txt")
basic_and_pretraining_and_glove_and_am_output = os.path.join(DATA_FOLDER, "pretrained_on_opensub_qa_moses_glove_try1_on_squad_dev_test_output_alternate_model_reranked.txt")

basic_output_on_squad_model_predictions = os.path.join(DATA_FOLDER, "moses_try1_on_squad_model_predictions_squad_dev_test_output.txt")
basic_and_pretraining_output_on_squad_model_predictions = os.path.join(DATA_FOLDER, "pretrained_on_opensub_qa_moses_try1_on_squad_model_predictions_squad_dev_test_output.txt")
basic_and_pretraining_and_glove_output_on_squad_model_predictions = os.path.join(DATA_FOLDER, "pretrained_on_opensub_qa_moses_glove_try1_on_squad_model_predictions_squad_dev_test_output.txt")
basic_and_pretraining_and_glove_and_lm_output_on_squad_model_predictions = os.path.join(DATA_FOLDER, "pretrained_on_opensub_qa_moses_glove_try1_on_squad_dev_test_squad_model_predictions_output_lm_reranked.txt")
basic_and_pretraining_and_glove_and_am_output_on_squad_model_predictions = os.path.join(DATA_FOLDER, "pretrained_on_opensub_qa_moses_glove_try1_on_squad_dev_test_squad_model_predictions_output_alternate_model_reranked.txt")
## LABELS
# decomposable attention model responses = d
# coqa model response = c
# our basic model response = b
# basic + pretraining = bp
# basic + pretraining + glove = bpg
# basic + pretraining + glove + lm = bpgl
# basic + pretraining + glove + am = bpga

# our basic model response + squad model predictions = b_s
# basic + pretraining + squad model predictions = bp_s
# basic + pretraining + glove + squad model predictions = bpg_s
# basic + pretraining + glove + lm + squad model predictions = bpgl_s
# basic + pretraining + glove + am + squad model predictions = bpga_s

id_q_a_list = read_dev_test_q_and_a(src_test_file)

def attach_label_to_qar_list(qar_list, label):
	# Also simultaneously check how many questions did each model get correct
	new_qar_list = list()
	correct_count = 0
	for tup in qar_list:
		_, q, a, r = tup
		a = a.lower()
		if a in r:
			correct_count += 1
		# elif label == 'quac':
		# 	# print(a, "::", r)
		# 	pass
		new_qar_list.append((*tup, label))
	print(label, ":", correct_count, "/", len(qar_list))
	return new_qar_list

b_qar_list = attach_label_to_qar_list(read_opennmt_output(basic_output), "b")
bp_qar_list = attach_label_to_qar_list(read_opennmt_output(basic_and_pretraining_output), "bp")
bpg_qar_list = attach_label_to_qar_list(read_opennmt_output(basic_and_pretraining_and_glove_output), "bpg")
bpgl_qar_list = attach_label_to_qar_list(read_opennmt_output(basic_and_pretraining_and_glove_and_lm_output), "bpgl")
bpga_qar_list = attach_label_to_qar_list(read_opennmt_output(basic_and_pretraining_and_glove_and_am_output), "bpga")


d_qar_list = attach_label_to_qar_list(read_decomposable_attn_output(decomposable_attn_model_output, id_q_a_list), "d")
c_qar_list = attach_label_to_qar_list(read_coqa_output(coqa_output), "c")
quac_qar_list = attach_label_to_qar_list(read_quac_output(quac_output), "quac")
b_s_qar_list = attach_label_to_qar_list(read_opennmt_output(basic_output_on_squad_model_predictions), "b_s")
bp_s_qar_list = attach_label_to_qar_list(read_opennmt_output(basic_and_pretraining_output_on_squad_model_predictions), "bp_s")
bpg_s_qar_list = attach_label_to_qar_list(read_opennmt_output(basic_and_pretraining_and_glove_output_on_squad_model_predictions), "bpg_s")
bpgl_s_qar_list = attach_label_to_qar_list(read_opennmt_output(basic_and_pretraining_and_glove_and_lm_output_on_squad_model_predictions), "bpgl_s")
bpga_s_qar_list = attach_label_to_qar_list(read_opennmt_output(basic_and_pretraining_and_glove_and_am_output_on_squad_model_predictions), "bpga_s")


all_qars = [d_qar_list, c_qar_list, quac_qar_list, b_qar_list, bp_qar_list, bpg_qar_list, bpgl_qar_list, bpga_qar_list]
# Trying to fix the mistake that I made for the first 2 batches
# all_qars = [d_qar_list]

# Getting the quac model outputs
all_qars = [quac_qar_list]

# Getting bpg on squad model prediction outputs
all_qars = [bpg_s_qar_list]

# Final batch with all squad model predictions
all_qars = [d_qar_list, c_qar_list, quac_qar_list, b_s_qar_list, bp_s_qar_list, bpg_s_qar_list, bpgl_s_qar_list, bpga_s_qar_list, bpga_qar_list]

# Group responses based on q,a
total_unique_response = 0
all_qar_dict = dict()
for id, q, a in id_q_a_list:
	# For each question we will aggregate same responses from different models into one instance. While appending the labels
	# Gather responses and labels from all models for current q,a
	current_responses_and_labels = dict()
	for qar_list in all_qars:
		qar_id, qar_q, qar_a, qar_response, qar_label = qar_list[id-1]
		if id != qar_id:
			print("Serious error in gathering!")
			exit()
		if qar_response in current_responses_and_labels:
			current_responses_and_labels[qar_response] += ":" + qar_label
		else:
			current_responses_and_labels[qar_response] = qar_label
	total_unique_response += len(current_responses_and_labels)
	all_qar_dict[q] = (id, a, current_responses_and_labels)
	# print(id, len(current_responses_and_labels))
print(total_unique_response, 9000)
print(len(all_qar_dict))

def convert_dict_to_array(qars):
	qars_list = list()
	# sort by ids
	sorted_qars = sorted(qars.items(), key=lambda kv: kv[1][0])
	# N = len(sorted_qars)
	# First batch
	N = 50
	# Temp
	# Second Batch
	N_start = 50
	N_end = 150
	# Found an issue here
	# D model 1 to 150 annotations are bogus since i was not loading the dict properly
	N_start = 0
	N_end = 150
	# Third Batch
	N_start = 150
	N_end = 300
	# Fourth Batch
	N_start = 300
	N_end = 450
	# Quac Output Batch 1
	N_start = 0
	N_end = 100
	# bpg squad Output Batch 1
	N_start = 0
	N_end = 100
	# New final batch 1
	N_start = 0
	N_end = 50
	# New final batch 2
	N_start = 50
	N_end = 150
	# New final batch 3
	N_start = 150
	N_end = 250
	# New final batch 4
	N_start = 250
	N_end = 350
	# New final batch 5
	N_start = 350
	N_end = 500
	# convert all instances into list and shuffle
	all_instances = list()
	instance_counts = 0
	for i in range(N_start, N_end):
		q, (id, a, responses_dict) = sorted_qars[i]
		sorted_responses_dict = sorted(responses_dict.items(), key=lambda kv: kv[0])
		for response, label in sorted_responses_dict:
			all_instances.append((id, q, a, response, label))
	random.shuffle(all_instances)
	print(len(all_instances))
	return all_instances



def save_qars_to_mturk_csv(qars, csv_save_file):
	n = 10
	with open(csv_save_file, "w") as csv_file:
		writer = csv.writer(csv_file, delimiter=',')
		writer.writerow(["id0","question0","answer0","response0","label0","id1","question1","answer1","response1","label1","id2","question2","answer2","response2","label2","id3","question3","answer3","response3","label3","id4","question4","answer4","response4","label4","id5","question5","answer5","response5","label5","id6","question6","answer6","response6","label6","id7","question7","answer7","response7","label7","id8","question8","answer8","response8","label8","id9","question9","answer9","response9","label9"])
		#TODO: continue here!
		all_qars_list = convert_dict_to_array(qars)
		# print_list(all_qars_list)
		for i in range(0, len(all_qars_list), n):
			hit_qars = all_qars_list[i:i+n]
			if len(hit_qars) < n:
				# fill the remaining from the top of the list
				hit_qars.extend(all_qars_list[0:n-len(hit_qars)])
				# print("yess:", len(hit_qars))
			hit_qars = list(sum([(id,q,a,r,label) for (id,q,a,r,label) in hit_qars], ()))
			writer.writerow(hit_qars)


output_csv = "batch_input.csv"
output_csv = "batch_input2.csv"
output_csv = "batch_d_input1-150.csv"
output_csv = "batch_input3.csv"
output_csv = "batch_input4.csv"
output_csv = "quac_batch_input1.csv"
output_csv = "bpg_s_batch_input1.csv"

output_csv = "final_batch1_new_format.csv"
output_csv = "final_batch2_new_format.csv"
output_csv = "final_batch3_new_format.csv"
output_csv = "final_batch4_new_format.csv"
output_csv = "final_batch5_new_format.csv"
save_qars_to_mturk_csv(all_qar_dict, output_csv)
# read_opennmt_output(basic_output)
# read_opennmt_output(basic_and_pretraining_output)
# read_opennmt_output(basic_and_pretraining_and_glove_output)
# read_opennmt_output(basic_and_pretraining_and_glove_and_lm_output)
# read_opennmt_output(basic_and_pretraining_and_glove_and_am_output)



